{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itl6NSdDW0lY"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. INSTALACIÓN DE DEPENDENCIAS\n",
        "# ==========================================\n",
        "!pip install tensorflow keras scikit-learn matplotlib pandas numpy opencv-python-headless\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.utils import resample, shuffle\n",
        "\n",
        "# Configuración de Semilla para Reproducibilidad\n",
        "def seed_everything(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "seed_everything()\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. CONEXIÓN CON DRIVE Y CARGA DE DATOS\n",
        "# ==========================================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/proyecto_completo/'\n",
        "DATA_PATH = os.path.join(BASE_PATH, 'preprocesamiento')\n",
        "MODELS_PATH = os.path.join(BASE_PATH, 'models')\n",
        "GAN_DATA_PATH = os.path.join(BASE_PATH, 'checkpoints') # Donde se guardó el .npy generado\n",
        "\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "\n",
        "# Cargar Datos Originales (Desbalanceados)\n",
        "print(\"Cargando datos originales...\")\n",
        "X_train_orig = np.load(os.path.join(DATA_PATH, 'X_train_unbalanced.npy'))\n",
        "y_train_orig = np.load(os.path.join(DATA_PATH, 'y_train_unbalanced.npy'))\n",
        "X_val = np.load(os.path.join(DATA_PATH, 'X_val_improved.npy'))\n",
        "y_val = np.load(os.path.join(DATA_PATH, 'y_val_improved.npy'))\n",
        "\n",
        "print(f\"X_train Original: {X_train_orig.shape}\")\n",
        "print(f\"Distribución Original: {np.bincount(y_train_orig.astype(int))}\")"
      ],
      "metadata": {
        "id": "pdVLUhL2W6PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. ESTRATEGIAS DE BALANCEO DE DATOS\n",
        "# ==========================================\n",
        "\n",
        "# --- A. RANDOM OVER SAMPLING (ROS) - Para Escenario 5 ---\n",
        "print(\"\\n--- Preparando Dataset ROS ---\")\n",
        "X_normal = X_train_orig[y_train_orig == 0]\n",
        "X_pneumonia = X_train_orig[y_train_orig == 1]\n",
        "\n",
        "# Duplicar clase minoritaria\n",
        "X_normal_ros = resample(X_normal, replace=True, n_samples=len(X_pneumonia), random_state=42)\n",
        "\n",
        "X_train_ros = np.concatenate((X_normal_ros, X_pneumonia))\n",
        "y_train_ros = np.concatenate((np.zeros(len(X_normal_ros)), np.ones(len(X_pneumonia))))\n",
        "X_train_ros, y_train_ros = shuffle(X_train_ros, y_train_ros, random_state=42)\n",
        "print(f\"Dataset ROS listo: {X_train_ros.shape}\")\n",
        "\n",
        "# --- B. DATA AUGMENTATION SINTÉTICO (GAN) - Para Escenarios 6 y 7 ---\n",
        "print(\"\\n--- Preparando Dataset GAN ---\")\n",
        "# Cargar datos generados en el Notebook 1\n",
        "gan_file = [f for f in os.listdir(GAN_DATA_PATH) if 'generated_data' in f and f.endswith('.npy')][0]\n",
        "synthetic_path = os.path.join(GAN_DATA_PATH, gan_file)\n",
        "print(f\"Cargando sintéticos desde: {synthetic_path}\")\n",
        "\n",
        "synthetic_imgs = np.load(synthetic_path)\n",
        "\n",
        "# Preprocesar sintéticos para que coincidan con ResNet (N, 224, 224, 3)\n",
        "if synthetic_imgs.ndim == 3: synthetic_imgs = np.expand_dims(synthetic_imgs, axis=-1)\n",
        "synthetic_imgs = synthetic_imgs.astype('float32') / 255.0 # Normalizar [0, 1]\n",
        "synthetic_imgs = np.repeat(synthetic_imgs, 3, axis=-1)    # Grayscale -> RGB\n",
        "\n",
        "# Crear etiquetas (Asumiendo que generamos la clase minoritaria 0)\n",
        "synthetic_labels = np.zeros(len(synthetic_imgs))\n",
        "\n",
        "# Concatenar\n",
        "X_train_gan = np.concatenate((X_train_orig, synthetic_imgs), axis=0)\n",
        "y_train_gan = np.concatenate((y_train_orig, synthetic_labels), axis=0)\n",
        "X_train_gan, y_train_gan = shuffle(X_train_gan, y_train_gan, random_state=42)\n",
        "\n",
        "print(f\"Dataset GAN listo: {X_train_gan.shape}\")"
      ],
      "metadata": {
        "id": "hCJaFVMlXDrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. FUNCIONES DE PÉRDIDA PERSONALIZADAS\n",
        "# ==========================================\n",
        "\n",
        "# --- FOCAL LOSS ---\n",
        "def focal_loss(gamma=2.0, alpha=0.25):\n",
        "    def focal_loss_fixed(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1. - K.epsilon())\n",
        "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
        "        alpha_factor = tf.where(tf.equal(y_true, 1), alpha, 1 - alpha)\n",
        "        return K.mean(-alpha_factor * K.pow(1 - pt, gamma) * K.log(pt))\n",
        "    return focal_loss_fixed\n",
        "\n",
        "# --- BWCCE (Balanced Weighted Cross Entropy) ---\n",
        "def bwce_loss(class_weights={0: 1.85, 1: 0.69}):\n",
        "    def bwce_loss_fixed(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.cast(y_pred, tf.float32)\n",
        "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        bce = -(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred))\n",
        "        weights = y_true * class_weights[1] + (1 - y_true) * class_weights[0]\n",
        "        return K.mean(weights * bce)\n",
        "    return bwce_loss_fixed\n",
        "\n",
        "# --- LDAM LOSS ---\n",
        "# Margen basado en frecuencias (Calculado previamente C=0.4)\n",
        "margin_normal = 0.04 # Aprox para ejemplo\n",
        "margin_pneumonia = 0.02\n",
        "def ldams_loss():\n",
        "    def ldams_loss_fixed(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1 - K.epsilon())\n",
        "        logit = tf.math.log(y_pred / (1 - y_pred))\n",
        "        margin = tf.where(tf.equal(y_true, 1.0), margin_pneumonia, margin_normal)\n",
        "        adjusted_logit = tf.where(tf.equal(y_true, 1.0), logit - margin, logit + margin)\n",
        "        exp_adjusted = tf.exp(adjusted_logit)\n",
        "        adjusted_pred = exp_adjusted / (exp_adjusted + 1.0)\n",
        "        bce = - (y_true * tf.math.log(adjusted_pred + K.epsilon()) +\n",
        "                 (1 - y_true) * tf.math.log(1 - adjusted_pred + K.epsilon()))\n",
        "        return K.mean(bce)\n",
        "    return ldams_loss_fixed"
      ],
      "metadata": {
        "id": "nQ8XUCAUXUbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. ARQUITECTURA CLASIFICADOR BASE\n",
        "# ==========================================\n",
        "def create_resnet50_model(input_shape=(224, 224, 3)):\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False # Congelar base\n",
        "\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    return Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "id": "9rAtA55lXZWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. CONFIGURACIÓN COMÚN\n",
        "# ==========================================\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "LR = 1e-4\n",
        "\n",
        "# Data Augmentation (Solo para Train)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15, width_shift_range=0.1, height_shift_range=0.1,\n",
        "    shear_range=0.1, zoom_range=0.1, horizontal_flip=True, fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Callbacks Base\n",
        "def get_callbacks(filename):\n",
        "    return [\n",
        "        ModelCheckpoint(os.path.join(MODELS_PATH, filename), save_best_only=True, monitor='val_accuracy', mode='max'),\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
        "    ]"
      ],
      "metadata": {
        "id": "MtPyJDKTXdmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. EJECUCIÓN DE ESCENARIOS (1 al 5)\n",
        "# ==========================================\n",
        "\n",
        "# --- ESCENARIO 1: BASE ---\n",
        "print(\"\\n--- Entrenando Escenario 1: BASE ---\")\n",
        "model_s1 = create_resnet50_model()\n",
        "model_s1.compile(optimizer=Adam(LR), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_s1.fit(train_datagen.flow(X_train_orig, y_train_orig, batch_size=BATCH_SIZE),\n",
        "             steps_per_epoch=len(X_train_orig)//BATCH_SIZE, epochs=EPOCHS,\n",
        "             validation_data=(X_val, y_val), callbacks=get_callbacks('resnet50_neumonia_final.h5'))\n",
        "\n",
        "# --- ESCENARIO 2: FOCAL LOSS ---\n",
        "print(\"\\n--- Entrenando Escenario 2: FOCAL ---\")\n",
        "model_s2 = create_resnet50_model()\n",
        "model_s2.compile(optimizer=Adam(LR), loss=focal_loss(), metrics=['accuracy'])\n",
        "model_s2.fit(train_datagen.flow(X_train_orig, y_train_orig, batch_size=BATCH_SIZE),\n",
        "             steps_per_epoch=len(X_train_orig)//BATCH_SIZE, epochs=EPOCHS,\n",
        "             validation_data=(X_val, y_val), callbacks=get_callbacks('resnet50_focal_loss_final.h5'))\n",
        "\n",
        "# --- ESCENARIO 3: BWCCE ---\n",
        "print(\"\\n--- Entrenando Escenario 3: BWCCE ---\")\n",
        "model_s3 = create_resnet50_model()\n",
        "model_s3.compile(optimizer=Adam(LR), loss=bwce_loss(), metrics=['accuracy'])\n",
        "model_s3.fit(train_datagen.flow(X_train_orig, y_train_orig, batch_size=BATCH_SIZE),\n",
        "             steps_per_epoch=len(X_train_orig)//BATCH_SIZE, epochs=EPOCHS,\n",
        "             validation_data=(X_val, y_val), callbacks=get_callbacks('resnet50_bwcc_final.h5'))\n",
        "\n",
        "# --- ESCENARIO 4: LDAM ---\n",
        "print(\"\\n--- Entrenando Escenario 4: LDAM ---\")\n",
        "model_s4 = create_resnet50_model()\n",
        "model_s4.compile(optimizer=Adam(LR), loss=ldams_loss(), metrics=['accuracy'])\n",
        "model_s4.fit(train_datagen.flow(X_train_orig, y_train_orig, batch_size=BATCH_SIZE),\n",
        "             steps_per_epoch=len(X_train_orig)//BATCH_SIZE, epochs=EPOCHS,\n",
        "             validation_data=(X_val, y_val), callbacks=get_callbacks('resnet50_ldam_final.h5'))\n",
        "\n",
        "# --- ESCENARIO 5: ROS ---\n",
        "print(\"\\n--- Entrenando Escenario 5: ROS ---\")\n",
        "model_s5 = create_resnet50_model()\n",
        "model_s5.compile(optimizer=Adam(LR), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_s5.fit(train_datagen.flow(X_train_ros, y_train_ros, batch_size=BATCH_SIZE),\n",
        "             steps_per_epoch=len(X_train_ros)//BATCH_SIZE, epochs=EPOCHS,\n",
        "             validation_data=(X_val, y_val), callbacks=get_callbacks('resnet50_ROS_best.h5'))"
      ],
      "metadata": {
        "id": "FOXCGvkNXoJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 8. ESCENARIO 6: BASE + GAN (Con Fine-Tuning)\n",
        "# ==========================================\n",
        "print(\"\\n--- Entrenando Escenario 6: GAN (Fase 1) ---\")\n",
        "model_s6 = create_resnet50_model()\n",
        "model_s6.compile(optimizer=Adam(LR), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fase 1: Entrenar cabeza\n",
        "hist_s6_1 = model_s6.fit(train_datagen.flow(X_train_gan, y_train_gan, batch_size=BATCH_SIZE),\n",
        "                         steps_per_epoch=len(X_train_gan)//BATCH_SIZE, epochs=EPOCHS,\n",
        "                         validation_data=(X_val, y_val),\n",
        "                         callbacks=get_callbacks('resnet50_Base_GAN_temp.h5'))\n",
        "\n",
        "print(\"\\n--- Entrenando Escenario 6: GAN (Fase 2 - Fine Tuning) ---\")\n",
        "# Descongelar últimas capas\n",
        "for layer in model_s6.layers[-30:]: layer.trainable = True\n",
        "\n",
        "model_s6.compile(optimizer=Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fase 2: Fine Tuning\n",
        "model_s6.fit(train_datagen.flow(X_train_gan, y_train_gan, batch_size=BATCH_SIZE),\n",
        "             steps_per_epoch=len(X_train_gan)//BATCH_SIZE, epochs=30,\n",
        "             validation_data=(X_val, y_val),\n",
        "             callbacks=get_callbacks('resnet50_Base_GAN_FINETUNED_best.h5'))"
      ],
      "metadata": {
        "id": "c58I1U7AXz37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 9. ESCENARIO 7: HÍBRIDO (FOCAL + GAN + Fine-Tuning)\n",
        "# ==========================================\n",
        "print(\"\\n--- Entrenando Escenario 7: HÍBRIDO (Fase 1) ---\")\n",
        "model_s7 = create_resnet50_model()\n",
        "model_s7.compile(optimizer=Adam(LR), loss=focal_loss(), metrics=['accuracy'])\n",
        "\n",
        "# Fase 1\n",
        "model_s7.fit(train_datagen.flow(X_train_gan, y_train_gan, batch_size=BATCH_SIZE),\n",
        "             steps_per_epoch=len(X_train_gan)//BATCH_SIZE, epochs=EPOCHS,\n",
        "             validation_data=(X_val, y_val),\n",
        "             callbacks=get_callbacks('resnet50_Focal_GAN_temp.h5'))\n",
        "\n",
        "print(\"\\n--- Entrenando Escenario 7: HÍBRIDO (Fase 2 - Fine Tuning) ---\")\n",
        "for layer in model_s7.layers[-30:]: layer.trainable = True\n",
        "\n",
        "# Nota: Pasamos el objeto de loss instanciado\n",
        "model_s7.compile(optimizer=Adam(1e-5), loss=focal_loss(), metrics=['accuracy'])\n",
        "\n",
        "# Fase 2\n",
        "model_s7.fit(train_datagen.flow(X_train_gan, y_train_gan, batch_size=BATCH_SIZE),\n",
        "             steps_per_epoch=len(X_train_gan)//BATCH_SIZE, epochs=30,\n",
        "             validation_data=(X_val, y_val),\n",
        "             callbacks=get_callbacks('resnet50_Focal_GAN_FINETUNED_best.h5'))\n",
        "\n",
        "print(\"¡Todos los escenarios entrenados y modelos guardados!\")"
      ],
      "metadata": {
        "id": "EZR8FKA_X7Vd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}