{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUUo8PTmYmWW"
      },
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# 1. SETUP Y LIBRERÍAS\n",
        "# ==========================================\n",
        "!pip install tensorflow keras scikit-learn matplotlib pandas numpy opencv-python-headless statsmodels scipy\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, matthews_corrcoef,\n",
        "                             recall_score, precision_score)\n",
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "import scipy.stats\n",
        "\n",
        "# Rutas\n",
        "BASE_PATH = '/content/drive/MyDrive/proyecto_completo/'\n",
        "DATA_PATH = os.path.join(BASE_PATH, 'preprocesamiento')\n",
        "MODELS_PATH = os.path.join(BASE_PATH, 'models')\n",
        "\n",
        "# Cargar Datos de Test\n",
        "print(\"Cargando Test Set...\")\n",
        "X_test = np.load(os.path.join(DATA_PATH, 'X_test_improved.npy'))\n",
        "y_test = np.load(os.path.join(DATA_PATH, 'y_test_improved.npy'))\n",
        "print(f\"Test Set cargado: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. DEFINICIÓN DE MODELOS A EVALUAR\n",
        "# ==========================================\n",
        "model_files = {\n",
        "    \"Esc1_Base\":    \"resnet50_neumonia_final.h5\",\n",
        "    \"Esc2_Focal\":   \"resnet50_focal_loss_final.h5\",\n",
        "    \"Esc3_BWCCE\":   \"resnet50_bwcc_final.h5\",\n",
        "    \"Esc4_LDAM\":    \"resnet50_ldam_final.h5\",\n",
        "    \"Esc5_ROS\":     \"resnet50_ROS_best.h5\",\n",
        "    \"Esc6_GAN\":     \"resnet50_Base_GAN_FINETUNED_best.h5\",\n",
        "    \"Esc7_Hibrido\": \"resnet50_Focal_GAN_FINETUNED_best.h5\"\n",
        "}\n",
        "\n",
        "# Verificar existencia\n",
        "for name, file in model_files.items():\n",
        "    path = os.path.join(MODELS_PATH, file)\n",
        "    exists = \"✅\" if os.path.exists(path) else \"X\"\n",
        "    print(f\"{exists} {name}: {file}\")"
      ],
      "metadata": {
        "id": "WDaE039nYs8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. FUNCIÓN DE CÁLCULO DE MÉTRICAS\n",
        "# ==========================================\n",
        "def calculate_extended_metrics(y_true, y_pred_prob):\n",
        "    # Convertir probabilidad a clase (umbral 0.5)\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    # Matriz de Confusión\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "\n",
        "    # Métricas Base\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "    # Métricas Clínicas\n",
        "    sensitivity = recall_score(y_true, y_pred) # Recall clase 1\n",
        "    specificity = tn / (tn + fp)               # Recall clase 0\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "\n",
        "    # Métricas Compuestas\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    mcc = matthews_corrcoef(y_true, y_pred)\n",
        "    g_mean = np.sqrt(sensitivity * specificity)\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, y_pred_prob)\n",
        "    except:\n",
        "        auc = 0.0\n",
        "\n",
        "    return {\n",
        "        \"Accuracy\": acc,\n",
        "        \"Bal_Accuracy\": bal_acc,\n",
        "        \"Sensitivity\": sensitivity,\n",
        "        \"Specificity\": specificity,\n",
        "        \"Precision\": precision,\n",
        "        \"F1_Score\": f1,\n",
        "        \"MCC\": mcc,\n",
        "        \"G_Mean\": g_mean,\n",
        "        \"AUC\": auc\n",
        "    }"
      ],
      "metadata": {
        "id": "jV-Twv0fYx--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. GENERACIÓN DE TABLA COMPARATIVA\n",
        "# ==========================================\n",
        "results_list = []\n",
        "predictions_dict = {} # Guardamos preds para los tests estadísticos\n",
        "\n",
        "print(\"Evaluando modelos...\")\n",
        "\n",
        "for name, filename in model_files.items():\n",
        "    path = os.path.join(MODELS_PATH, filename)\n",
        "    if not os.path.exists(path): continue\n",
        "\n",
        "    # Cargar sin compilar (más rápido y seguro)\n",
        "    model = load_model(path, compile=False)\n",
        "\n",
        "    # Predecir\n",
        "    probs = model.predict(X_test, verbose=0)\n",
        "    if probs.shape[1] > 1: probs = probs[:, 1] # Si es softmax\n",
        "    else: probs = probs.ravel()                # Si es sigmoid\n",
        "\n",
        "    # Guardar para uso posterior\n",
        "    predictions_dict[name] = probs\n",
        "\n",
        "    # Calcular métricas\n",
        "    metrics = calculate_extended_metrics(y_test, probs)\n",
        "    metrics[\"Model\"] = name\n",
        "    results_list.append(metrics)\n",
        "\n",
        "    print(f\"-> {name} procesado.\")\n",
        "    del model\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "# Crear DataFrame\n",
        "df_results = pd.DataFrame(results_list)\n",
        "cols = [\"Model\", \"MCC\", \"Bal_Accuracy\", \"Sensitivity\", \"Specificity\", \"F1_Score\", \"AUC\"]\n",
        "df_final = df_results[cols].sort_values(by=\"MCC\", ascending=False)\n",
        "\n",
        "print(\"\\n=== TABLA FINAL DE RESULTADOS ===\")\n",
        "display(df_final)\n",
        "df_final.to_csv(os.path.join(BASE_PATH, 'tabla_resultados_finales.csv'), index=False)"
      ],
      "metadata": {
        "id": "PyJHCPKMY1vA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. TESTS ESTADÍSTICOS (MCNEMAR & DELONG)\n",
        "# ==========================================\n",
        "\n",
        "# --- A. TEST DE MCNEMAR ---\n",
        "def run_mcnemar(y_true, prob_a, prob_b):\n",
        "    pred_a = (prob_a > 0.5).astype(int)\n",
        "    pred_b = (prob_b > 0.5).astype(int)\n",
        "\n",
        "    # Tabla de contingencia\n",
        "    c_a = (pred_a == y_true)\n",
        "    c_b = (pred_b == y_true)\n",
        "    n01 = np.sum(~c_a & c_b) # A falla, B acierta\n",
        "    n10 = np.sum(c_a & ~c_b) # A acierta, B falla\n",
        "\n",
        "    # Cálculo exacto\n",
        "    table = [[0, n10], [n01, 0]] # Solo importan los desacuerdos\n",
        "    res = mcnemar([[0, n10], [n01, 0]], exact=True)\n",
        "    return res.pvalue\n",
        "\n",
        "# --- B. TEST DE DELONG (Matemática Pura) ---\n",
        "def compute_midrank(x):\n",
        "    J = np.argsort(x)\n",
        "    Z = x[J]\n",
        "    N = len(x)\n",
        "    T = np.zeros(N, dtype=np.float64)\n",
        "    i = 0\n",
        "    while i < N:\n",
        "        j = i\n",
        "        while j < N and Z[j] == Z[i]: j += 1\n",
        "        T[i:j] = 0.5 * (i + j - 1)\n",
        "        i = j\n",
        "    T2 = np.empty(N, dtype=np.float64)\n",
        "    T2[J] = T + 1\n",
        "    return T2\n",
        "\n",
        "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
        "    m = label_1_count\n",
        "    n = predictions_sorted_transposed.shape[1] - m\n",
        "    positive_examples = predictions_sorted_transposed[:, :m]\n",
        "    negative_examples = predictions_sorted_transposed[:, m:]\n",
        "    k = predictions_sorted_transposed.shape[0]\n",
        "\n",
        "    tx = np.empty([k, m], dtype=np.float64)\n",
        "    ty = np.empty([k, n], dtype=np.float64)\n",
        "    tz = np.empty([k, m + n], dtype=np.float64)\n",
        "\n",
        "    for r in range(k):\n",
        "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
        "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
        "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
        "\n",
        "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
        "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
        "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
        "    sx = np.cov(v01)\n",
        "    sy = np.cov(v10)\n",
        "    delongcov = sx / m + sy / n\n",
        "    return aucs, delongcov\n",
        "\n",
        "def run_delong(y_true, prob_a, prob_b):\n",
        "    order = np.argsort(y_true)[::-1]\n",
        "    y_sorted = y_true[order]\n",
        "    num_pos = np.sum(y_sorted == 1)\n",
        "\n",
        "    preds_A = prob_a[order]\n",
        "    preds_B = prob_b[order]\n",
        "\n",
        "    data = np.array([preds_A, preds_B])\n",
        "    aucs, cov = fastDeLong(data, num_pos)\n",
        "\n",
        "    l = np.array([[1, -1]])\n",
        "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, cov), l.T))\n",
        "    p_val = 2.0 * (1.0 - scipy.stats.norm.cdf(z[0][0]))\n",
        "    return p_val\n",
        "\n",
        "# --- EJECUCIÓN COMPARATIVA (Vs GAN) ---\n",
        "ref_model = \"Esc6_GAN\" # modelo estrella\n",
        "if ref_model in predictions_dict:\n",
        "    print(f\"\\nCOMPARACIONES ESTADÍSTICAS (Referencia: {ref_model})\")\n",
        "    print(f\"{'MODELO':<15} | {'McNemar p':<12} | {'DeLong p':<12} | {'Significancia'}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    for name, probs in predictions_dict.items():\n",
        "        if name == ref_model: continue\n",
        "\n",
        "        p_mc = run_mcnemar(y_test, predictions_dict[ref_model], probs)\n",
        "        p_dl = run_delong(y_test, predictions_dict[ref_model], probs)\n",
        "\n",
        "        sig = \"SIGNIFICATIVO\" if p_mc < 0.05 or p_dl < 0.05 else \"NO Sig.\"\n",
        "        print(f\"{name:<15} | {p_mc:.4f}       | {p_dl:.4f}       | {sig}\")"
      ],
      "metadata": {
        "id": "6Bhz_9sWY_XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. AUDITORÍA DE EXPLICABILIDAD (ROI GRAD-CAM)\n",
        "# ==========================================\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return cv2.resize(heatmap.numpy(), (224, 224))\n",
        "\n",
        "def ejecutar_auditoria_roi(model_name):\n",
        "    print(f\"\\nAuditando explicabilidad de: {model_name}...\")\n",
        "    path = os.path.join(MODELS_PATH, model_files[model_name])\n",
        "    model = load_model(path, compile=False)\n",
        "\n",
        "    # Filtrar solo clase Pneumonia (1)\n",
        "    indices_pneumonia = np.where(y_test == 1)[0]\n",
        "    hits = 0\n",
        "\n",
        "    # ROI (Margen 5%)\n",
        "    margin = int(224 * 0.05)\n",
        "\n",
        "    for idx in indices_pneumonia:\n",
        "        img = X_test[idx]\n",
        "        img_input = np.expand_dims(img, axis=0)\n",
        "\n",
        "        try:\n",
        "            heatmap = make_gradcam_heatmap(img_input, model, 'conv5_block3_out')\n",
        "            if np.max(heatmap) == 0: continue\n",
        "\n",
        "            # Punto de máxima atención\n",
        "            max_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
        "            cY, cX = max_idx\n",
        "\n",
        "            # Verificar si cae en zona central (ROI Pulmonar)\n",
        "            if (margin < cX < 224-margin) and (margin < cY < 224-margin):\n",
        "                hits += 1\n",
        "        except: continue\n",
        "\n",
        "    score = (hits / len(indices_pneumonia)) * 100\n",
        "    print(f\"Robustez de Atención (ROI Score): {score:.2f}% ({hits}/{len(indices_pneumonia)})\")\n",
        "\n",
        "# Ejecutar auditoría en el modelo GAN\n",
        "if \"Esc6_GAN\" in model_files:\n",
        "    ejecutar_auditoria_roi(\"Esc6_GAN\")"
      ],
      "metadata": {
        "id": "iHvUAHQDZKXs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}