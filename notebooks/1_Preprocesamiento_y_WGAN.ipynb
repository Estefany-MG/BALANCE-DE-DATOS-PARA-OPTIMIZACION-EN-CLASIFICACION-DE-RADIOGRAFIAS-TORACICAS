{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. INSTALACIÓN DE DEPENDENCIAS\n",
        "# ==========================================\n",
        "!pip install tensorflow keras scikit-learn matplotlib pandas numpy opencv-python-headless\n",
        "!pip install torch torchvision fastprogress kaggle\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from google.colab import drive\n",
        "import random\n",
        "\n",
        "# Semilla para reproducibilidad\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed_everything()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Librerías listas. Usando dispositivo: {device}\")"
      ],
      "metadata": {
        "id": "cRe99CHRU427"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 2. CONEXIÓN CON GOOGLE DRIVE\n",
        "# ==========================================\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta Base del Proyecto (AJUSTA ESTO SI ES NECESARIO)\n",
        "BASE_PATH = '/content/drive/MyDrive/proyecto_completo/'\n",
        "\n",
        "# Carpetas de salida\n",
        "DATA_PATH = os.path.join(BASE_PATH, 'preprocesamiento')\n",
        "MODELS_PATH = os.path.join(BASE_PATH, 'models')\n",
        "CHECKPOINT_PATH = os.path.join(BASE_PATH, 'checkpoints')\n",
        "\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Directorio de trabajo configurado en: {BASE_PATH}\")"
      ],
      "metadata": {
        "id": "LYTikzIeSRon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. BIBLIOTECA DE PREPROCESAMIENTO (UTILS)\n",
        "# ==========================================\n",
        "\n",
        "IMG_WIDTH, IMG_HEIGHT = 256, 256  # Tamaño intermedio\n",
        "RESNET_SIZE = 224                 # Tamaño final\n",
        "\n",
        "def center_crop(img, crop_size=None):\n",
        "    \"\"\"Recorta el centro de la imagen para eliminar bordes irrelevantes.\"\"\"\n",
        "    if crop_size is None:\n",
        "        crop_size = int(min(img.shape[0], img.shape[1]) * 0.95)\n",
        "\n",
        "    y, x = img.shape[:2]\n",
        "    startx = x // 2 - crop_size // 2\n",
        "    starty = y // 2 - crop_size // 2\n",
        "    startx = max(0, startx)\n",
        "    starty = max(0, starty)\n",
        "\n",
        "    return img[starty:starty+crop_size, startx:startx+crop_size]\n",
        "\n",
        "def apply_clahe(img):\n",
        "    \"\"\"Aplica CLAHE. Si es RGB, convierte a LAB, aplica a L y reconvierte.\"\"\"\n",
        "    if len(img.shape) == 3 and img.shape[2] == 3:\n",
        "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        cl = clahe.apply(l)\n",
        "        limg = cv2.merge((cl, a, b))\n",
        "        return cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
        "    else:\n",
        "        # Escala de grises\n",
        "        if len(img.shape) == 3: img = img[:,:,0]\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        return clahe.apply(img)\n",
        "\n",
        "def apply_gaussian_filter(img, kernel_size=(3, 3)):\n",
        "    \"\"\"Suavizado para reducir ruido.\"\"\"\n",
        "    return cv2.GaussianBlur(img, kernel_size, 0)\n",
        "\n",
        "def preprocess_image(img, target_size=(224, 224)):\n",
        "    \"\"\"Pipeline completo de preprocesamiento.\"\"\"\n",
        "    try:\n",
        "        # 1. Center Crop\n",
        "        img = center_crop(img)\n",
        "        # 2. Resize intermedio\n",
        "        img = cv2.resize(img, (256, 256))\n",
        "        # 3. CLAHE\n",
        "        img = apply_clahe(img)\n",
        "        # 4. Gaussian\n",
        "        img = apply_gaussian_filter(img)\n",
        "        # 5. Resize final\n",
        "        img = cv2.resize(img, target_size)\n",
        "\n",
        "        # Normalización [0, 1]\n",
        "        img = img.astype(np.float32) / 255.0\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocesando: {e}\")\n",
        "        return None\n",
        "\n",
        "def analyze_dataset(labels):\n",
        "    \"\"\"Calcula el Desbalance (IR).\"\"\"\n",
        "    counter = Counter(labels)\n",
        "    majority = max(counter.values())\n",
        "    minority = min(counter.values())\n",
        "    ir = majority / minority\n",
        "    print(f\"Distribución: {counter}\")\n",
        "    print(f\"Imbalance Ratio (IR): {ir:.2f}\")\n",
        "    return ir"
      ],
      "metadata": {
        "id": "5uplP8oxVLC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4. PREPARACIÓN DE DATOS PARA WGAN\n",
        "# ==========================================\n",
        "\n",
        "# Cargar datos originales\n",
        "print(\"Cargando arrays .npy originales...\")\n",
        "try:\n",
        "    X_train = np.load(os.path.join(DATA_PATH, 'X_train_unbalanced.npy'))\n",
        "    y_train = np.load(os.path.join(DATA_PATH, 'y_train_unbalanced.npy'))\n",
        "    print(f\"Datos cargados. Shape: {X_train.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"No se encontraron los archivos .npy en la ruta especificada.\")\n",
        "\n",
        "\n",
        "def ensure_grayscale_clahe_and_norm(X, target_channels=1):\n",
        "    \"\"\"\n",
        "    Asegura formato (N, H, W, 1), aplica CLAHE y normaliza a [-1, 1] para la GAN.\n",
        "    \"\"\"\n",
        "    print(\"Aplicando preprocesamiento específico para WGAN (CLAHE + Norm [-1,1])...\")\n",
        "    X_processed = []\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        img = X[i]\n",
        "        # Asegurar uint8 para CLAHE\n",
        "        if img.dtype != np.uint8:\n",
        "            if img.max() <= 1.0: img = (img * 255).astype(np.uint8)\n",
        "            else: img = img.astype(np.uint8)\n",
        "\n",
        "        # Si es RGB, pasar a Gris\n",
        "        if len(img.shape) == 3 and img.shape[2] == 3:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        elif len(img.shape) == 3 and img.shape[2] == 1:\n",
        "            img = img[:, :, 0]\n",
        "\n",
        "        # Aplicar CLAHE\n",
        "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "        img = clahe.apply(img)\n",
        "        X_processed.append(img)\n",
        "\n",
        "    X_processed = np.array(X_processed)\n",
        "\n",
        "    # Expandir dims (N, H, W, 1)\n",
        "    if X_processed.ndim == 3:\n",
        "        X_processed = np.expand_dims(X_processed, axis=-1)\n",
        "\n",
        "    # Normalizar a [-1, 1] (Requisito de Generator con Tanh)\n",
        "    X_processed = (X_processed.astype('float32') - 127.5) / 127.5\n",
        "\n",
        "    return X_processed\n",
        "\n",
        "# Procesar\n",
        "X_train_wgan = ensure_grayscale_clahe_and_norm(X_train)\n",
        "\n",
        "# Filtrar Clase Minoritaria (Normal=0, Pneumonia=1)\n",
        "counts = np.bincount(y_train.astype(int))\n",
        "minority_class = np.argmin(counts)\n",
        "print(f\"Clase minoritaria detectada: {minority_class} (Cantidad: {counts[minority_class]})\")\n",
        "\n",
        "X_minority = X_train_wgan[y_train == minority_class]\n",
        "print(f\"Dataset para WGAN listo: {X_minority.shape}, Rango: [{X_minority.min():.1f}, {X_minority.max():.1f}]\")"
      ],
      "metadata": {
        "id": "9BR8Pu4WVWC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. ARQUITECTURA WGAN-GP (PyTorch)\n",
        "# ==========================================\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=128, channels=1, img_size=224):\n",
        "        super().__init__()\n",
        "        self.init_size = img_size // 32\n",
        "        self.inp = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 512 * self.init_size ** 2),\n",
        "            nn.BatchNorm1d(512 * self.init_size ** 2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        def sn_conv_t(in_c, out_c):\n",
        "            return nn.utils.spectral_norm(\n",
        "                nn.ConvTranspose2d(in_c, out_c, 4, stride=2, padding=1, bias=False))\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            sn_conv_t(512, 256), nn.BatchNorm2d(256), nn.ReLU(True),\n",
        "            sn_conv_t(256, 128), nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            sn_conv_t(128, 64),  nn.BatchNorm2d(64),  nn.ReLU(True),\n",
        "            sn_conv_t(64, 32),   nn.BatchNorm2d(32),  nn.ReLU(True),\n",
        "            sn_conv_t(32, channels),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, z):\n",
        "        x = self.inp(z)\n",
        "        x = x.view(x.size(0), 512, self.init_size, self.init_size)\n",
        "        return self.main(x)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, channels=1, img_size=224):\n",
        "        super().__init__()\n",
        "        def sn_conv(in_c, out_c):\n",
        "            return nn.utils.spectral_norm(\n",
        "                nn.Conv2d(in_c, out_c, 4, stride=2, padding=1, bias=False))\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            sn_conv(channels, 32), nn.LeakyReLU(0.2, inplace=True),\n",
        "            sn_conv(32, 64),   nn.InstanceNorm2d(64),  nn.LeakyReLU(0.2, inplace=True),\n",
        "            sn_conv(64, 128),  nn.InstanceNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n",
        "            sn_conv(128, 256), nn.InstanceNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n",
        "            sn_conv(256, 512), nn.InstanceNorm2d(512), nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512 * (img_size // 32) ** 2, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.out(self.main(x))"
      ],
      "metadata": {
        "id": "LfZAvcw2VkFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6. ENTRENAMIENTO PROGRESIVO (3 ETAPAS)\n",
        "# ==========================================\n",
        "import torch.autograd as autograd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Configuración Progresiva ---\n",
        "STAGES = [64, 128, 224]  # Las 3 resoluciones\n",
        "EPOCHS_PER_STAGE = [500, 500, 4000] # Según la Tabla 12\n",
        "BATCH_SIZES = [64, 32, 32] # Ajustar según memoria (64 para pequeña, 32 para grande)\n",
        "\n",
        "# Hiperparámetros WGAN-GP\n",
        "LR_G = 1e-4\n",
        "LR_C = 5e-5 # (En la última etapa ajustaremos esto dinámicamente)\n",
        "N_CRITIC = 5\n",
        "LAMBDA_GP = 10.0\n",
        "LATENT_DIM = 128\n",
        "\n",
        "# Tensor de referencia\n",
        "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "\n",
        "# --- Dataset que redimensiona dinámicamente ---\n",
        "class ProgressiveDataset(Dataset):\n",
        "    def __init__(self, x_data, current_size):\n",
        "        self.x_data = x_data\n",
        "        self.current_size = current_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convertir (H, W, 1) -> (1, H, W) -> Tensor\n",
        "        img = self.x_data[idx]\n",
        "        img_tensor = torch.from_numpy(img.transpose(2, 0, 1))\n",
        "        # Redimensionar a la etapa actual (ej. 64x64)\n",
        "        img_resized = F.interpolate(img_tensor.unsqueeze(0), size=(self.current_size, self.current_size), mode='bilinear', align_corners=False)\n",
        "        return img_resized.squeeze(0)\n",
        "\n",
        "# --- Función de Penalización de Gradiente ---\n",
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates)\n",
        "    fake = Tensor(real_samples.shape[0], 1).fill_(1.0)\n",
        "    gradients = autograd.grad(\n",
        "        outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
        "        create_graph=True, retain_graph=True, only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    return ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "# --- BUCLE PRINCIPAL DE ETAPAS ---\n",
        "for i, img_size in enumerate(STAGES):\n",
        "    print(f\"\\n\" + \"=\"*40)\n",
        "    print(f\" INICIANDO ETAPA {i+1}: Resolución {img_size}x{img_size}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # 1. Configurar Datos para esta etapa\n",
        "    dataset = ProgressiveDataset(X_target, current_size=img_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZES[i], shuffle=True, drop_last=True)\n",
        "\n",
        "    # 2. Instanciar Modelos para esta resolución\n",
        "    generator = Generator(img_size=img_size).to(device)\n",
        "    critic = Critic(img_size=img_size).to(device)\n",
        "\n",
        "    # 3. Cargar pesos de la etapa anterior (Transfer Learning Progresivo)\n",
        "    if i > 0:\n",
        "        prev_gen_path = os.path.join(CHECKPOINT_PATH, f'generator_stage{i}_best.pth')\n",
        "        prev_crit_path = os.path.join(CHECKPOINT_PATH, f'critic_stage{i}_best.pth')\n",
        "\n",
        "        if os.path.exists(prev_gen_path):\n",
        "            print(f\"Cargando pesos de etapa previa: {prev_gen_path}\")\n",
        "            # Carga flexible (strict=False) para adaptar capas de distinto tamaño\n",
        "            pretrained_dict = torch.load(prev_gen_path)\n",
        "            model_dict = generator.state_dict()\n",
        "            # Filtrar solo pesos que coincidan en forma\n",
        "            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and v.shape == model_dict[k].shape}\n",
        "            model_dict.update(pretrained_dict)\n",
        "            generator.load_state_dict(model_dict)\n",
        "\n",
        "            # Lo mismo para el crítico\n",
        "            critic.load_state_dict(torch.load(prev_crit_path), strict=False)\n",
        "\n",
        "    # 4. Ajustar Learning Rates (Escalado en etapa final)\n",
        "    current_lr_g = LR_G * 0.5 if i == 2 else LR_G\n",
        "    current_lr_c = LR_C * 1.5 if i == 2 else LR_C # Ajuste según tu código (7.5e-5 vs 5e-5)\n",
        "\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=current_lr_g, betas=(0.0, 0.9))\n",
        "    optimizer_C = optim.Adam(critic.parameters(), lr=current_lr_c, betas=(0.0, 0.9))\n",
        "\n",
        "    # 5. Entrenamiento de la Etapa\n",
        "    epochs = EPOCHS_PER_STAGE[i]\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        for batch_idx, imgs in enumerate(dataloader):\n",
        "\n",
        "            real_imgs = imgs.type(Tensor).to(device)\n",
        "\n",
        "            # --- Entrenar Crítico ---\n",
        "            optimizer_C.zero_grad()\n",
        "            z = Tensor(np.random.normal(0, 1, (imgs.shape[0], LATENT_DIM)))\n",
        "            fake_imgs = generator(z)\n",
        "\n",
        "            real_validity = critic(real_imgs)\n",
        "            fake_validity = critic(fake_imgs)\n",
        "            gp = compute_gradient_penalty(critic, real_imgs.data, fake_imgs.data)\n",
        "\n",
        "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + LAMBDA_GP * gp\n",
        "            d_loss.backward()\n",
        "            optimizer_C.step()\n",
        "\n",
        "            # --- Entrenar Generador (cada n_critic pasos) ---\n",
        "            if batch_idx % N_CRITIC == 0:\n",
        "                optimizer_G.zero_grad()\n",
        "                fake_imgs = generator(z) # Regenerar para grafo computacional\n",
        "                fake_validity = critic(fake_imgs)\n",
        "                g_loss = -torch.mean(fake_validity)\n",
        "                g_loss.backward()\n",
        "                optimizer_G.step()\n",
        "\n",
        "        # Log simple\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"[Etapa {img_size}x{img_size}] Epoch {epoch}/{epochs} | D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    # 6. Guardar Modelo de la Etapa\n",
        "    torch.save(generator.state_dict(), os.path.join(CHECKPOINT_PATH, f'generator_stage{i+1}_best.pth'))\n",
        "    torch.save(critic.state_dict(), os.path.join(CHECKPOINT_PATH, f'critic_stage{i+1}_best.pth'))\n",
        "    print(f\"Modelo de Etapa {img_size}x{img_size} guardado.\")\n",
        "\n",
        "# Guardar el modelo final definitivo (Stage 3) como 'best_generator_stage3.pth' para la generación\n",
        "torch.save(generator.state_dict(), os.path.join(CHECKPOINT_PATH, 'best_generator_stage3.pth'))\n",
        "print(\"Entrenamiento progresivo completo.\")"
      ],
      "metadata": {
        "id": "HmLYTt0WVx1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. GENERACIÓN DE IMÁGENES SINTÉTICAS\n",
        "# ==========================================\n",
        "LATENT_DIM = 128\n",
        "IMG_SIZE = 224\n",
        "NUM_IMGS_TO_GEN = 2179\n",
        "\n",
        "best_gen_path = os.path.join(CHECKPOINT_PATH, 'best_generator_stage3.pth')\n",
        "\n",
        "if os.path.exists(best_gen_path):\n",
        "    generator = Generator(img_size=IMG_SIZE).to(device)\n",
        "    generator.load_state_dict(torch.load(best_gen_path, map_location=device))\n",
        "    generator.eval()\n",
        "\n",
        "    print(f\"Generando {NUM_IMGS_TO_GEN} imágenes sintéticas...\")\n",
        "    synthetic_imgs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(0, NUM_IMGS_TO_GEN, 32):\n",
        "            curr = min(32, NUM_IMGS_TO_GEN - len(synthetic_imgs))\n",
        "            if curr <= 0: break\n",
        "            noise = torch.randn(curr, LATENT_DIM, device=device)\n",
        "            gen = generator(noise).cpu().numpy()\n",
        "            # Desnormalizar de [-1, 1] -> [0, 255]\n",
        "            gen = (gen + 1) / 2.0 * 255.0\n",
        "            synthetic_imgs.append(gen)\n",
        "\n",
        "    synthetic_data = np.concatenate(synthetic_imgs, axis=0).astype(np.uint8).squeeze()\n",
        "\n",
        "    save_path = os.path.join(CHECKPOINT_PATH, f'generated_data_{NUM_IMGS_TO_GEN}.npy')\n",
        "    np.save(save_path, synthetic_data)\n",
        "    print(f\"Guardado: {save_path}\")"
      ],
      "metadata": {
        "id": "Hj-Q26dLVqty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}